{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c784f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Opening JSON file\n",
    "f = open('Client_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f53c65",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(f)\n",
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['_source'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79907c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "df2 = json_normalize(df._source)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543412f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()/len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bd87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc20a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['company'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41deb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['complaint_what_happened'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73012489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['issue'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b5a2a",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c3b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b192ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['complaint_what_happened'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['product']=df2['product'].apply(lambda s:preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045688fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['complaint_what_happened']=df2['complaint_what_happened'].apply(lambda s:preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['sub_product']=df2['sub_product'].apply(lambda s:preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c04988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdff2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = []\n",
    "df2[\"all_text\"] = df2[\"product\"]+df2[\"sub_product\"]+df2[\"complaint_what_happened\"]\n",
    "word_data = df2[\"all_text\"].apply(lambda x:word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde788c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57abcab2",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fe18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc15567",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af71f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in word_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50973e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words corpus for our sample document that is  (token_id, token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07094098",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel(corpus=corpus,id2word=dictionary,num_topics=5,passes=50,chunksize=1500,iterations=200,alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68edd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lda_model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topics(num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46779556",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0146f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(x):\n",
    "    max_tup = (0,0)\n",
    "    for i in x:\n",
    "        if i[1]>max_tup[1]:\n",
    "            max_tup = i\n",
    "    return max_tup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for i in corpus:\n",
    "    topics.append(extract(model.get_document_topics(i)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc248d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame([\"all_text\",\"type\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b53a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sample(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d58d1",
   "metadata": {},
   "source": [
    "# create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bcc702",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final.all_text\n",
    "y = df_final.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify = y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23259bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba47b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97760cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b82162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
